
# IoT Data Pipeline with Kafka, PySpark, and InfluxDB

## Overview
This application processes IoT data generated by RFID readers, streams it through Kafka, processes it in PySpark, and stores results in InfluxDB. Notifications are triggered if certain thresholds (e.g., high wait times) are exceeded.

---

## Prerequisites
1. **Docker** (for Kafka, InfluxDB, and Grafana):
   - Ensure `docker-compose` is installed.
2. **Python**:
   - Version 3.11+ with required libraries (requirements.txt) installed.
3. **Apache Spark**:
   - Installed locally or use a cluster (e.g., AWS EMR, Databricks).

---

## Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/your-repo/iot-data-pipeline.git
cd iot-data-pipeline
```

### 2. Configure the Environment
- Update `docker-compose.yml` (if needed) for Kafka, InfluxDB, and Grafana setup.
- Update `spark_stream_to_influxdb.py` with:
  - Kafka bootstrap server details.
  - InfluxDB credentials.

### 3. Start Docker Services
Bring up Kafka, InfluxDB, and Grafana using Docker Compose:
```bash
docker-compose up -d
```

Verify services:
- **Kafka**: Available at `localhost:9092`.
- **InfluxDB**: Available at `http://localhost:8086` (default credentials: `admin/admin_password`).
- **Grafana**: Available at `http://localhost:3000` (default credentials: `admin/admin`).

---

## Running the Application

### 1. Run the MQTT to Kafka Bridge
This script simulates or collects RFID data and streams it to Kafka.
```bash
python mqtt_to_kafka_bridge.py
```

### 2. Run the Spark Streaming Script
This script processes Kafka messages, aggregates data, and writes it to InfluxDB.
```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 spark_stream_to_influxdb.py
```

---

## Dashboard Setup

### 1. Access Grafana
- Go to `http://localhost:3000` and log in with default credentials.
- Add **InfluxDB** as a data source:
  - URL: `http://influxdb:8086`.
  - Database: `iot_data`.
  - Credentials: Use InfluxDB credentials set during setup.

### 2. Create a Real-Time Dashboard
- Add panels for metrics like:
  - **Wait Times**: Average wait time by zone.
  - **Heatmap**: Activity density by zone and time.

---

## Testing

### Kafka Message Simulation
- Use the Kafka console consumer to check data:
  ```bash
  kafka-console-consumer --bootstrap-server localhost:9092 --topic iot-stream --from-beginning
  ```

### Verify InfluxDB Data
- Log in to the InfluxDB UI at `http://localhost:8086`.
- Query the `iot_data` bucket to ensure processed data is stored.

### Simulate High Wait Times
- Modify `mqtt_to_kafka_bridge.py` to generate high wait times for testing notification triggers.

---

## Stop the Services
To stop all running services:
```bash
docker-compose down
```

---

## Next Steps
- Extend the pipeline to include more IoT devices or analytics.
- Optimize PySpark aggregations for high-throughput environments.
- Add user notification preferences for more flexibility.
